{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ecb0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import bleedfacedetector as fd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8681a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /Users/xperience/actions-runner/_work/opencv-python/opencv-python/opencv/modules/dnn/src/onnx/onnx_importer.cpp:260: error: (-210:Unsupported format or combination of formats) Failed to parse ONNX model: ../emotion-ferplus-8.onnx in function 'ONNXImporter'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_k/xbbdqcr54b3bqjfpdw47pfs80000gn/T/ipykernel_17568/2767340185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../emotion-ferplus-8.onnx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Now read the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNetFromONNX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /Users/xperience/actions-runner/_work/opencv-python/opencv-python/opencv/modules/dnn/src/onnx/onnx_importer.cpp:260: error: (-210:Unsupported format or combination of formats) Failed to parse ONNX model: ../emotion-ferplus-8.onnx in function 'ONNXImporter'\n"
     ]
    }
   ],
   "source": [
    "# Set model path\n",
    "model = '../emotion-ferplus-8.onnx'\n",
    "# Now read the model\n",
    "net = cv2.dnn.readNetFromONNX(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50dffd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# The gender model architecture\n",
    "# https://drive.google.com/open?id=1W_moLzMlGiELyPxWiYQJ9KFaXroQ_NFQ\n",
    "GENDER_MODEL = 'deploy_gender.prototxt.txt'\n",
    "# The gender model pre-trained weights\n",
    "# https://drive.google.com/open?id=1AW3WduLk1haTVAxHOkVS_BEzel1WXQHP\n",
    "GENDER_PROTO = 'gender_net.caffemodel'\n",
    "# Each Caffe Model impose the shape of the input image also image preprocessing is required like mean\n",
    "# substraction to eliminate the effect of illunination changes\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "# Represent the gender classes\n",
    "GENDER_LIST = ['Male', 'Female']\n",
    "# https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n",
    "FACE_PROTO = \"deploy.prototxt.txt\"\n",
    "# https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "FACE_MODEL = \"res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "\n",
    "face_net = cv2.dnn.readNetFromCaffe(FACE_PROTO, FACE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "782c435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_net = cv2.dnn.readNetFromCaffe(GENDER_MODEL, GENDER_PROTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f7fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faces(frame, confidence_threshold=0.5):\n",
    "    # convert the frame into a blob to be ready for NN input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104, 177.0, 123.0))\n",
    "    # set the image as input to the NN\n",
    "    face_net.setInput(blob)\n",
    "    # perform inference and get predictions\n",
    "    output = np.squeeze(face_net.forward())\n",
    "    # initialize the result list\n",
    "    faces = []\n",
    "    # Loop over the faces detected\n",
    "    for i in range(output.shape[0]):\n",
    "        confidence = output[i, 2]\n",
    "        if confidence > confidence_threshold:\n",
    "            box = output[i, 3:7] * \\\n",
    "                np.array([frame.shape[1], frame.shape[0],\n",
    "                         frame.shape[1], frame.shape[0]])\n",
    "            # convert to integers\n",
    "            start_x, start_y, end_x, end_y = box.astype(np.int)\n",
    "            # widen the box a little\n",
    "            start_x, start_y, end_x, end_y = start_x - \\\n",
    "                10, start_y - 10, end_x + 10, end_y + 10\n",
    "            start_x = 0 if start_x < 0 else start_x\n",
    "            start_y = 0 if start_y < 0 else start_y\n",
    "            end_x = 0 if end_x < 0 else end_x\n",
    "            end_y = 0 if end_y < 0 else end_y\n",
    "            # append to our list\n",
    "            faces.append((start_x, start_y, end_x, end_y))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68db104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(title, img):\n",
    "    \"\"\"Displays an image on screen and maintains the output until the user presses a key\"\"\"\n",
    "    # Display Image on screen\n",
    "    cv2.imshow(title, img)\n",
    "    # Mantain output until user presses a key\n",
    "    cv2.waitKey(0)\n",
    "    # Destroy windows when user presses a key\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7988fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_font_scale(text, width):\n",
    "    \"\"\"Determine the optimal font scale based on the hosting frame width\"\"\"\n",
    "    for scale in reversed(range(0, 60, 1)):\n",
    "        textSize = cv2.getTextSize(text, fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=scale/10, thickness=1)\n",
    "        new_width = textSize[0][0]\n",
    "        if (new_width <= width):\n",
    "            return scale/10\n",
    "    return 1\n",
    "\n",
    "# from: https://stackoverflow.com/questions/44650888/resize-an-image-without-distortion-opencv\n",
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "    # resize the image\n",
    "    return cv2.resize(image, dim, interpolation = inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adc544a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(input_path):\n",
    "    return cv2.imread(input_path)\n",
    "\n",
    "def predict_gender(img):\n",
    "    faces = get_faces(img)\n",
    "    if len(faces)>1:\n",
    "        return None\n",
    "    #import pdb; pdb.set_trace()\n",
    "    start_x, start_y, end_x, end_y = faces[0]\n",
    "    face_img = img[start_y: end_y, start_x: end_x]\n",
    "    blob = cv2.dnn.blobFromImage(image=face_img, scalefactor=1.0, size=(\n",
    "            227, 227), mean=MODEL_MEAN_VALUES, swapRB=False, crop=False)\n",
    "    gender_net.setInput(blob)\n",
    "    gender_preds = gender_net.forward()\n",
    "    #['Male', 'Female']\n",
    "    return gender_preds[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db1fece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/xbbdqcr54b3bqjfpdw47pfs80000gn/T/ipykernel_17568/1631374886.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  start_x, start_y, end_x, end_y = box.astype(np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = read_image('/Users/joanna/research/denoising_flows/examples/detect_gender/man.png')\n",
    "predict_gender(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d285f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
